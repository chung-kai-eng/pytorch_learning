{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d7cdc8-ffaf-4348-ba1d-ddfdb5a70d3b",
   "metadata": {},
   "source": [
    "[DQN explanation](https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a09f79a-1af2-4029-b72a-94db82a386d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path: C:\\Users\\DALab\\Documents\\GitHub\\pytorch_learning\\Reinforcement Learning\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "print('current path: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7af282-ad83-4ea5-98c1-666318c8c9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc9106b-9188-44f3-bd83-5d1228837a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cf4a2-5e2f-47f2-8f9a-a92e4286a73d",
   "metadata": {},
   "source": [
    "### Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7f19df-8c63-4be6-8478-59e9f9bb2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b5802-073a-4147-9a17-7275225b31f6",
   "metadata": {},
   "source": [
    "### Input Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b477ac-0e75-45e9-a24b-27d9b8d44845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DALab\\anaconda3\\envs\\dl_test\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl_test\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyglet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     38\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m---> 39\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mget_screen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m     40\u001b[0m            interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExample extracted screen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mget_screen\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_screen\u001b[39m():\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Returned screen requested by gym is 400x600x3, but is sometimes larger\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# such as 800x1200x3. Transpose it into torch order (CHW).\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Cart is in the lower half, so strip off the top and bottom of the screen\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     _, screen_height, screen_width \u001b[38;5;241m=\u001b[39m screen\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl_test\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:179\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m cartheight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30.0\u001b[39m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rendering\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m rendering\u001b[38;5;241m.\u001b[39mViewer(screen_width, screen_height)\n\u001b[0;32m    182\u001b[0m     l, r, t, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcartwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cartwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cartheight \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39mcartheight \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl_test\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Cannot import pyglet.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    HINT: you can install pyglet directly via 'pip install pyglet'.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    But if you really just want to install all Gym dependencies and not have to think about it,\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    'pip install -e .[all]' or 'pip install gym[all]' will do it.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: \n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "226cd0f8-2f31-4f50-87e2-042115d463e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms  \n",
    "\n",
    "def make_env(env_name, mode='train'):\n",
    "    env = gym.make(env_name)\n",
    "    env = FrameStack(env)\n",
    "    # different reward\n",
    "    if mode == 'train':\n",
    "        env = SurvivalRewardEnv(env)\n",
    "    return env\n",
    "\n",
    "def image_processing(state, num_frame=4):\n",
    "\n",
    "    print('print state shape: ', state.shape)\n",
    "            \n",
    "    transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize(size=(84,84)),\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    transforms.ToTensor()\n",
    "                ])\n",
    "    \n",
    "    for i in range(state.shape[-1]//3):\n",
    "        image = state[:,:, i*3:i*3+3]\n",
    "        transformed_image = transform(image)\n",
    "        \n",
    "        if i == 0:\n",
    "            stack_frames = transformed_image\n",
    "        else:\n",
    "            stack_frames = torch.cat((stack_frames, transformed_image), axis=0)\n",
    "        \n",
    "    print('stack frame shape: ', stack_frames.shape)\n",
    "    return stack_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e59595db-2a0a-46f0-b160-48e90b2ff10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print state shape:  (84, 168, 12)\n",
      "stack frame shape:  torch.Size([4, 84, 84])\n",
      "torch.Size([4, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(84, 168, 12).astype(np.uint8)\n",
    "stack_frames = image_processing(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a0fd6-40f9-40c3-ac94-8c3bd295d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfae38f-7966-45ad-b899-e7e9586bee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(state):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((84,84)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "    new_img = transform(state)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12faf200-5435-47f3-be4b-3e235dde775d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518c5f4-e2d5-4895-9587-9a068272a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "BN(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d08352-2cdd-4ab0-8623-0f29fc359034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced489ff-fbe0-4afb-8be0-2b687a4420c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
